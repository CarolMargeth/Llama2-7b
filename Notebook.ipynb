{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "\n",
    "import gradio as gr\n",
    "import spaces\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 1024\n",
    "MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\n",
    "\n",
    "DESCRIPTION = \"\"\"\\\n",
    "# Llama-2 7B Chat\n",
    "\n",
    "This Space demonstrates model [Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) by Meta, a Llama 2 model with 7B parameters fine-tuned for chat instructions. Feel free to play with it, or duplicate to run generations without a queue! If you want to run your own service, you can also [deploy the model on Inference Endpoints](https://huggingface.co/inference-endpoints).\n",
    "\n",
    "ðŸ”Ž For more details about the Llama 2 family of models and how to use them with `transformers`, take a look [at our blog post](https://huggingface.co/blog/llama2).\n",
    "\n",
    "ðŸ”¨ Looking for an even more powerful model? Check out the [13B version](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat) or the large [70B model demo](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI).\n",
    "\"\"\"\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p/>\n",
    "\n",
    "---\n",
    "As a derivate work of [Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) by Meta,\n",
    "this demo is governed by the original [license](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/main/LICENSE.txt) and [acceptable use policy](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/main/USE_POLICY.md).\n",
    "\"\"\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    DESCRIPTION += \"\\n<p>Running on CPU ðŸ¥¶ This demo does not work on CPU.</p>\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "\n",
    "@spaces.GPU\n",
    "def generate(\n",
    "    message: str,\n",
    "    chat_history: list[tuple[str, str]],\n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.2,\n",
    ") -> Iterator[str]:\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        {\"input_ids\": input_ids},\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs)\n",
    "\n",
    "\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=generate,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(label=\"System prompt\", lines=6),\n",
    "        gr.Slider(\n",
    "            label=\"Max new tokens\",\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Temperature\",\n",
    "            minimum=0.1,\n",
    "            maximum=4.0,\n",
    "            step=0.1,\n",
    "            value=0.6,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.9,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top-k\",\n",
    "            minimum=1,\n",
    "            maximum=1000,\n",
    "            step=1,\n",
    "            value=50,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Repetition penalty\",\n",
    "            minimum=1.0,\n",
    "            maximum=2.0,\n",
    "            step=0.05,\n",
    "            value=1.2,\n",
    "        ),\n",
    "    ],\n",
    "    stop_btn=None,\n",
    "    examples=[\n",
    "        [\"Hello there! How are you doing?\"],\n",
    "        [\"Can you explain briefly to me what is the Python programming language?\"],\n",
    "        [\"Explain the plot of Cinderella in a sentence.\"],\n",
    "        [\"How many hours does it take a man to eat a Helicopter?\"],\n",
    "        [\"Write a 100-word article on 'Benefits of Open-Source in AI research'\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "with gr.Blocks(css=\"style.css\") as demo:\n",
    "    gr.Markdown(DESCRIPTION)\n",
    "    gr.DuplicateButton(value=\"Duplicate Space for private use\", elem_id=\"duplicate-button\")\n",
    "    chat_interface.render()\n",
    "    gr.Markdown(LICENSE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue(max_size=20).launch()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
